import json
import re
import pandas as pd
from openai import OpenAI
import time

"""
This script defines a class to interact with the OpenAI API for evaluating software modules against CVE (Common Vulnerabilities and Exposures) descriptions.
It includes functionalities to:
- Load and parse configuration files and JSON data.
- Replace placeholders in prompt templates.
- Search for CVE descriptions.
- Check module vulnerability using the OpenAI API.
- Validate the API response.

Classes:
- GptCVEChecker: Manages the interaction with the OpenAI API and evaluates module vulnerabilities.

Functions:
- load_cve_json: Loads CVE data from a JSON file.
- load_prompt: Loads the prompt template from a file.
- replace_placeholders: Replaces placeholders in the prompt template with actual values.
- search_cve_description: Searches for a CVE description in a CSV file.
- check_vulnerability: Checks if a module is vulnerable using the OpenAI API.
- validate_response: Validates the structure and data types of the API response.
- call_open_api: Calls the OpenAI API to get a response.
- evaluate_module: Evaluates if a module is vulnerable using the OpenAI API.

Usage:
- Initialize GptCVEChecker with the path to the prompt template and CVE descriptions.
- Call evaluate_module with the CVE ID, module, and version to get the evaluation result.
"""

class GptCVEChecker:
    def __init__(self, prompt_file, cve_descriptions):
        # Initialize the OpenAI API client, prompt template, and CVE descriptions
        self.gpt = OpenAI() 
        self.prompt = self.load_prompt(prompt_file)
        self.cve_descriptions = cve_descriptions
        self.max_retries = 3

    def load_cve_json(self, json_file_path):
        # Load CVE data from a JSON file
        with open(json_file_path, 'r') as f:
            cve_data = json.load(f)
        return cve_data["cves"]

    def load_prompt(self, prompt_file):
        # Load the prompt template from a file
        try:
            with open(prompt_file, "r") as file:
                prompt = file.read()
            return prompt
        except FileNotFoundError:
            print("The file was not found at the specified path.")
            return None
        except Exception as e:
            print("An error occurred while reading the file:", e)
            return None

    def replace_placeholders(self, cve_id, module, version, cve_description):
        # Replace placeholders in the prompt template with actual values
        prompt = self.prompt
        prompt = prompt.replace("<CVE-ID>", cve_id)
        prompt = prompt.replace("<Module>", module)
        prompt = prompt.replace("<Version>", version)
        prompt = prompt.replace("<CVE-DESCRIPTION>", cve_description)
        return prompt

    def search_cve_description(self, cve_id):
        # Search for a CVE description in a CSV file
        df = pd.read_csv(self.cve_descriptions)
        description = df.loc[df['CVE-ID'] == cve_id, 'DESCRIPTION'].values[0]
        return description

    def check_vulnerability(self, prompt):
        # Check if a module is vulnerable using the OpenAI API
        retries = 0
        while retries < self.max_retries:
            response, fail = self.call_open_api(prompt)
            try:
                result = json.loads(response)  # Try to parse the response as JSON
                if "IsVulnerable" in result:
                    if isinstance(result["IsVulnerable"], str):
                        result["IsVulnerable"] = eval(result["IsVulnerable"])
                status = self.validate_response(result)
                return result, (status and not fail)  # If successful, return the result and status
            except json.JSONDecodeError:
                retries += 1
                if retries >= self.max_retries:
                    raise Exception("Failed to parse JSON after multiple retries.")  # Exceeded retry limit

    def validate_response(self, json_response):
        # Validate the structure and data types of the API response
        if "CVE-ID" not in json_response or "Module" not in json_response or "IsVulnerable" not in json_response or "Version" not in json_response:
            print("Not found Keys")
            return False

        if not isinstance(json_response["CVE-ID"], str) or not isinstance(json_response["Module"], str) or not isinstance(json_response["Version"], str) or not isinstance(json_response["IsVulnerable"], bool):
            print("Bad types")
            return False

        return True
    
    def call_open_api(self, cve_prompt):
        # Call the OpenAI API to get a response
        fail = False
        generated_text = ""
        try:
            response = self.gpt.chat.completions.create(
                model="gpt-3.5-turbo-0125",
                response_format={ "type": "json_object" },
                max_tokens=3000,
                messages=[
                    {"role": "system", "content": "You are a helpful assistant designed to output JSON."},
                    {"role": "user", "content": cve_prompt}
                ]
            )
            generated_text = response.choices[0].message.content
        except Exception as e:
            retry_time = 2
            print(f"Error {e}. Retrying in {retry_time} seconds...")
            time.sleep(retry_time)
            return self.call_open_api(cve_prompt)
        return generated_text, fail
    
    def evaluate_module(self, cve_id, module, version):
        # Evaluate if a module is vulnerable using the OpenAI API
        cve_description = self.search_cve_description(cve_id)
        prompt = self.replace_placeholders(cve_id, module, version, cve_description)
        evaluating = True
        result = {"IsVulnerable": False}  
        retryLimit = 3
        retryCount = 1
        while evaluating:
            if retryCount > retryLimit:
                print(f"Aborting because retry limit {retryLimit} and current retry count {retryCount}:\n{result}")
                result = {"IsVulnerable": False} 
                evaluating = False
            result, success = self.check_vulnerability(prompt)
            evaluating = not success
            if evaluating:
                print(f"Retrying because bad result in run {retryCount}:\n{result}")
                retryCount += 1  
        return result["IsVulnerable"], success

# Example usage
if __name__ == "__main__":
    prompt_file = "/Users/andresvargasrivera/repos/cve-automations/open_ai_api/templates_gpt/isVulnerableVersions.txt"
    cve_descriptions = "/Users/andresvargasrivera/repos/cve-automations/database/cves/2023_cves-purged.csv"
    cve_checker = GptCVEChecker(prompt_file, cve_descriptions)

    # 1. Read the JSON file of modules to evaluate
    cve_data = cve_checker.load_cve_json("/Users/andresvargasrivera/repos/cve-automations/open_ai_api/examples-openai-test.json")
    for cve in cve_data:
        cve_id = cve["CVE-ID"]
        module = cve["isVulnerable"]["Module"]
        version = cve["isVulnerable"]["Version"]
        cve_description = cve_checker.search_cve_description(cve_id)
        print(f"\nCVE-ID: {cve_id}, Module: {module}, Version: {version}")
        result = cve_checker.evaluate_module(cve_id, module, version)
