import os
import subprocess
from experimentGeneration import UploadDataExperimentManager
from modulesSearchExperiment import ModulesSearchExperiment
from directComparisonManager import DirectComparisonManager

"""
This script provides a console-based user interface for running various experiments related to CVE (Common Vulnerabilities and Exposures) data.
It includes functionalities to:
- Generate and clean databases.
- Validate and evaluate module names.
- Run training evaluations.
- Set configuration paths.

Classes:
- ExperimentConsoleUI: Manages the console interface and experiment execution.

Functions:
- display_menu: Displays the menu with available experiments.
- run: Main loop to handle user input and execute selected experiments.
- run_database_generation: Runs the database generation experiment.
- run_modules_validation: Runs the module names validation experiment.
- run_training_test: Runs the training evaluation experiment.
- set_config_path: Sets the CONFIG_PATH environment variable.

Usage:
- Run the script and follow the console prompts to select and execute experiments.
"""

class ExperimentConsoleUI:
    def __init__(self):
        # Initialize available experiments
        self.experiments = {
            1: "Database generation",
            2: "Modules names validation",
            3: "Modules Evaluation",
            4: "Training Evaluation",
            5: "Set CONFIG_PATH environment variable",
            6: "Exit"
        }

    def display_menu(self):
        # Display the menu with available experiments
        print("==== Experiment Console UI ====")
        print("Select an experiment to run:")
        for num, experiment in self.experiments.items():
            print(f"{num}. {experiment}")

    def run(self):
        # Main loop to handle user input and execute selected experiments
        while True:
            self.display_menu()
            choice = input("Enter your choice: ")
            if choice.isdigit():
                choice = int(choice)
                if choice == 1:
                    self.run_database_generation()
                elif choice == 2:
                    self.run_modules_validation()
                elif choice == 3:
                    self.run_modules_validation(True)
                elif choice == 4:
                    self.run_training_test()
                elif choice == 5:
                    self.set_config_path()
                elif choice == 6:
                    print("Exiting...")
                    break
                else:
                    print("Invalid choice. Please select a valid option.")
            else:
                print("Invalid input. Please enter a number.")

    def run_database_generation(self):
        # Run the database generation experiment
        # Ensure CONFIG_PATH and OPENAI_API_KEY environment variables are set
        config_path = os.getenv("CONFIG_PATH")
        experiment_manager = UploadDataExperimentManager(config_path)
        experiment_manager.clean_collections()
        experiment_manager.run_experiment()

    def run_modules_validation(self, evaluate=False):
        # Run the module names validation experiment
        config_path = os.getenv("CONFIG_PATH")
        analyzer = ModulesSearchExperiment(config_path)
        analyzer.analyze_experiment(evaluate)
    
    def run_training_test(self):
        # Run the training evaluation experiment
        config_path = os.getenv("CONFIG_PATH")
        for experiment in range(5):
            analyzer = DirectComparisonManager(config_path)
            analyzer.print_vulnerability_summary()
            analyzer.compare_vulnerabilities(experiment)

    def set_config_path(self):
        # Set the CONFIG_PATH environment variable
        config_path = input("Enter the path to the configuration file: ")
        os.environ["CONFIG_PATH"] = config_path
        print("CONFIG_PATH environment variable set successfully.")

if __name__ == "__main__":
    ui = ExperimentConsoleUI()
    ui.run()
