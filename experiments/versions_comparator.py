import re
import pymongo
import sys
import operator
import os
import ast
import pandas as pd
import time
from enum import Enum
from string_compare import *
import openai_cve_tester
from joblib import Parallel, delayed

"""
This script defines various functions and classes for analyzing vulnerabilities in software modules using different comparison types.
It includes functionalities to:
- Compare version strings using different algorithms.
- Analyze module vulnerabilities using heuristic and OpenAI methods.
- Run bulk experiments on multiple modules and databases.
- Calculate and store precision, recall, and other metrics for comparison.

Classes:
- ItemType: Enum to classify different items (operator, version, all).
- MetricType: Enum to classify different metrics (recall, precision, F1).
- ComparatorType: Enum to classify different comparator types (heuristic, OpenAI using DB, OpenAI full).

Functions:
- analyzeManifest: Analyzes the version manifest for a module.
- compareVersions: Compares version strings directly or completely.
- compare: Uses operators to compare versions.
- compareEqVersions: Compares versions for equality.
- compareCompleteManifest: Compares the entire version manifest.
- checkVersionString: Checks and formats version strings.
- checkVulnerability: Checks if a version module is vulnerable.
- identifyElements: Identifies elements in a version manifest.
- getCvesMatchModule: Retrieves CVEs matching a module from the database.
- getRatio: Gets the similarity ratio between modules using different algorithms.
- analyzeEachModule: Analyzes each module for vulnerabilities.
- executeAnalyzeModules: Executes analysis for modules based on comparator type.
- analyzeVulnerabilities: Analyzes vulnerabilities for modules.
- checkModule: Checks modules for vulnerabilities and calculates metrics.
- getMetric: Calculates precision and recall.
- runBulkModuleExperiment: Runs bulk experiments on multiple modules.
- saveAnalysis: Saves analysis results to a CSV file.
- resultsAnalyzis: Analyzes the results of experiments.
- runDatabaseAnalysis: Runs analysis on multiple databases.
- readExperimentConfiguration: Reads experiment configuration and runs analysis.
- main: Main function to run the script based on command-line arguments.
"""

ops = {
    '<': operator.lt,
    '<=': operator.le,
    '>': operator.gt,
    '>=': operator.ge,
    '=': operator.eq,
    '==': operator.eq,
}

class ItemType(Enum):
    OPERATOR = 'operator'
    VERSION = 'version'
    ALL = 'all'

class MetricType(Enum):
    RECALL = 'RECALL'
    PRECISION = 'PRECISION'
    F1 = 'F1'

class ComparatorType(Enum):
    HEURISTIC = 'HEURISTIC'
    OPENAIDB = 'OPENAI_USING_DB'
    OPENAIFULL = 'OPENAI_FULL'

def analyzeManifest(versionModule, cveManifest):
    # Analyze the version manifest for a module
    if type(cveManifest) == list:
        cveManifest = ' '.join(cveManifest)
    versionModule = versionModule.lower()
    cveManifest = cveManifest.lower()
    cveManifest = checkVersionString(cveManifest)
    cveManifest = cveManifest.split()
    cveManifest, manifestType = identifyElements(cveManifest)
    vulnerable = checkVulnerability(cveManifest, manifestType, versionModule)
    return vulnerable

def compareVersions(cveManifest, versionModule, directCompare=True):
    # Compare version strings directly or completely
    vulnerable = False
    if not directCompare:
        vulnerable = compareCompleteManifest(cveManifest, versionModule)
    else:
        vulnerable = compareEqVersions(cveManifest, versionModule)
    return vulnerable

def compare(operator, cveVersion, moduleVersion):
    # Use operators to compare versions
    return ops[operator](moduleVersion, cveVersion)

def compareEqVersions(cveManifest, versionModule):
    # Compare versions for equality
    vulnerable = False
    for item in cveManifest:
        if item["tag"] == ItemType.VERSION.value:
            currentVersion = item["item"].replace(" ", "")
            vulnerable = vulnerable or operator.eq(currentVersion, versionModule.replace(" ", ""))
    return vulnerable

def compareCompleteManifest(cveManifest, versionModule):
    # Compare the entire version manifest
    operatorReady = None
    vulnerable = False
    firstTime = True
    for idx, item in enumerate(cveManifest):
        if item["tag"] == ItemType.OPERATOR.value:
            operatorReady = item["item"]
        elif item["tag"] == ItemType.VERSION.value and operatorReady:
            if firstTime:
                vulnerable = True
                firstTime = False
            currentVersion = item["item"].replace(" ", "")
            vulnerable = vulnerable and compare(operatorReady.replace(" ", ""), currentVersion, versionModule.replace(" ", ""))
            operatorReady = None
        elif item["tag"] == ItemType.ALL.value and operatorReady:
            vulnerable = True
    return vulnerable

def checkVersionString(version):
    # Check and format version strings
    operator = ''
    isValid = False
    currentOperatorIdx = 0
    for idx, letter in enumerate(version):
        if letter == '<' or letter == '>' or letter == '=':
            operator = operator + letter
            currentOperatorIdx = currentOperatorIdx + 1
        elif letter == ' ' and operator != '':
            break
        elif letter != ' ' and operator != '':
            isValid = True
            break
    res = operator + ' ' + version[currentOperatorIdx:]
    if not isValid:
        res = version
    return res

def checkVulnerability(cveManifest, manifestType, versionModule):
    # Check if a version module is vulnerable
    vulnerable = False
    if manifestType == ItemType.ALL:
        vulnerable = True
    elif manifestType == ItemType.VERSION:
        vulnerable = compareVersions(cveManifest, versionModule)
    elif manifestType == ItemType.OPERATOR:
        vulnerable = compareVersions(cveManifest, versionModule, directCompare=False)
    return vulnerable

def identifyElements(cveManifest):
    # Identify elements in a version manifest
    cveManifestTag = []
    manifestType = ItemType.ALL
    for item in cveManifest:
        if re.match("[<><=>=]", item):
            cveManifestTag.append({"item": item, "tag": ItemType.OPERATOR.value})
            manifestType = ItemType.OPERATOR
        elif re.match(".*[0-9].*", item):
            cveManifestTag.append({"item": item, "tag": ItemType.VERSION.value})
            if manifestType != ItemType.OPERATOR:
                manifestType = ItemType.VERSION
        else:
            cveManifestTag.append({"item": item, "tag": ItemType.ALL.value})
    return cveManifestTag, manifestType

def getCvesMatchModule(module, databaseName):
    # Retrieve CVEs matching a module from the database
    myclient = pymongo.MongoClient("mongodb+srv://afelipevargasr:9TC5hlN9CdC3czP6@cluster0.lrohl7y.mongodb.net/?retryWrites=true&w=majority")
    mydb = myclient["cve_simplifications"]
    mycol = mydb[databaseName]
    myquery = {"vulnerable_versions.module": {"$regex": "(?i)" + module}}
    cves = mycol.find()
    return cves

def getRatio(cveModule, currentModule):
    # Get the similarity ratio between modules using different algorithms
    matchRatio = compare_string(cveModule, currentModule, AlgorithmType.MATCHER)
    matchRatioL = compare_string(cveModule, currentModule, AlgorithmType.LEVENSHTEIN)
    matchRatioC = compare_string(cveModule, currentModule, AlgorithmType.COSINE)
    print("MATCHER ", matchRatio, flush=True)
    print("LEVENSHTEIN ", matchRatioL, flush=True)
    print("COSINE ", matchRatioC, flush=True)
    return matchRatio

def analyzeEachModule(cve, vendor, module, version, tolerationRate):
    # Analyze each module for vulnerabilities
    vulnerable = False
    passComparison = False
    for item in cve["vulnerable_versions"]:
        moduleName = module
        matchRatioC = getRatio(item["module"].lower(), moduleName)
        print(cve["CVE_ID"], flush=True)
        if matchRatioC < tolerationRate:
            moduleName = vendor + " " + module
            vendorMatchRatioC = getRatio(item["module"].lower(), moduleName)
            if vendorMatchRatioC >= tolerationRate:
                passComparison = True
        elif matchRatioC >= tolerationRate:
            passComparison = True

        if passComparison:
            print("tested version ", version, flush=True)
            print("module version ", item["versions"], flush=True)
            isModuleVulnerable = analyzeManifest(version, item["versions"])
            print("isModuleVulnerable ", isModuleVulnerable, flush=True)
            vulnerable = vulnerable or isModuleVulnerable
    return vulnerable

def executeAnalyzeModules(cve, vendor, module, version, tolerationRate, comparator):
    # Execute analysis for modules based on comparator type
    vulnerable = False
    if comparator == ComparatorType.HEURISTIC:
        vulnerable = analyzeEachModule(cve, vendor, module, version, tolerationRate)
    elif comparator == ComparatorType.OPENAIDB:
        vulnerable = openai_cve_tester.check_database(
            cve["CVE_ID"],
            module, version,
            openai_cve_tester.ComparationType.VERSIONS,
            cve["vulnerable_versions"])
    elif comparator == ComparatorType.OPENAIFULL:
        vulnerable = openai_cve_tester.check_database(
            cve["CVE_ID"],
            module, version,
            openai_cve_tester.ComparationType.DESCRIPTION)
    return vulnerable

def analyzeVulnerabilities(vendor, module, version, databaseName, tolerationRate, comparator):
    # Analyze vulnerabilities for modules
    vendor = vendor.lower()
    module = module.lower()
    version = version.lower()
    cves = getCvesMatchModule(module, databaseName)
    cvesVulnerable = []
    cvesNotVulnerable = []
    totalTime = 0
    cves_count = 0
    avgTime = 0
    for cve in cves:
        cves_count = cves_count + 1
        print("CVE: ", cve["CVE_ID"], flush=True)
        cve_st = time.time()
        vulnerable = executeAnalyzeModules(cve, vendor, module, version, tolerationRate, comparator)
        cve_et = time.time()
        elapsed_time = cve_et - cve_st
        totalTime = totalTime + elapsed_time
        if vulnerable:
            cvesVulnerable.append(cve["CVE_ID"])
        else:
            cvesNotVulnerable.append(cve["CVE_ID"])
    print("lenCVES ", cves_count, flush=True)
    if cves_count > 0:
        avgTime = totalTime / cves_count
    return cvesVulnerable, cvesNotVulnerable, totalTime, avgTime

def checkModule(vendor, module, version, realVulnerabilities, database, comparator):
    # Check modules for vulnerabilities and calculate metrics
    db = database["db"]
    tolerance = database["tolerance"]
    realVulnerabilities = ast.literal_eval(realVulnerabilities)
    vulnerableCVES, notVulnerableCVES, total, avg = analyzeVulnerabilities(vendor, module, version, db, tolerance, comparator)
    TruePositives = list(set(vulnerableCVES).intersection(realVulnerabilities))
    TrueNegatives = list(set(notVulnerableCVES) - set(realVulnerabilities))
    FalseNegatives = list(set(realVulnerabilities) - set(vulnerableCVES))
    FalsePositives = list(set(vulnerableCVES) - set(realVulnerabilities))
    print("type vulnerableCVES: ", type(vulnerableCVES))
    print("type notVulnerableCVES: ", type(notVulnerableCVES))
    print("type realVulnerabilities: ", type(realVulnerabilities))
    print("type TruePositives: ", type(TruePositives))
    print("realVulnerabilities ", realVulnerabilities, flush=True)
    print("vulnerableCVES ", vulnerableCVES, flush=True)
    print("notVulnerableCVES ", notVulnerableCVES, flush=True)
    print("TruePositives ", TruePositives, flush=True)
    print("TrueNegatives ", TrueNegatives, flush=True)
    print("FalseNegatives ", FalseNegatives, flush=True)
    print("FalsePositives ", FalsePositives, flush=True)
    passComparison = (realVulnerabilities == len(vulnerableCVES))

    resData = {
        "db": database,
        "tolerance": tolerance,
        "comparator": comparator,
        "module": module,
        "version": version,
        "realVulnerabilities": realVulnerabilities,
        "TruePositives": TruePositives,
        "TrueNegatives": TrueNegatives,
        "FalsePositives": FalsePositives,
        "FalseNegatives": FalseNegatives
    }
    resLen = {
        "db": database,
        "tolerance": tolerance,
        "comparator": comparator,
        "module": module,
        "version": version,
        "totalTime": total,
        "avgTime": avg,
        "lenRealVulnerabilities": len(realVulnerabilities),
        "lenTruePositives": len(TruePositives),
        "lenTrueNegatives": len(TrueNegatives),
        "lenFalsePositives": len(FalsePositives),
        "lenFalseNegatives": len(FalseNegatives),
        "precision": getMetric(len(TruePositives), len(FalsePositives), len(FalseNegatives), MetricType.PRECISION),
        "recall": getMetric(len(TruePositives), len(FalsePositives), len(FalseNegatives), MetricType.RECALL),
        "passComparison": len(TruePositives) == len(realVulnerabilities)
    }
    return resData, resLen

def getMetric(TruePositives, FalsePositives, FalseNegatives, metric):
    # Calculate precision and recall
    res = 0
    if TruePositives == 0 and FalseNegatives == 0 and FalsePositives == 0:
        res = 1
    elif (TruePositives > 0 or FalsePositives > 0) and metric == MetricType.PRECISION:
        res = TruePositives / (TruePositives + FalsePositives)
    elif (TruePositives > 0 or FalseNegatives > 0) and metric == MetricType.RECALL:
        res = TruePositives / (TruePositives + FalseNegatives)
    return res

def runBulkModuleExperiment(modules, database, comparator, tempDir, experiment):
    # Run bulk experiments on multiple modules
    comparisonNumberResults = pd.DataFrame(columns=["db", "tolerance", "comparator", "module", "version", "totalTime", "avgTime", "lenRealVulnerabilities", "lenTruePositives", "lenTrueNegatives", "lenFalsePositives", "lenFalseNegatives", "precision", "recall", "passComparison"])
    comparisonDataResults = pd.DataFrame(columns=["db", "tolerance", "comparator", "module", "version", "realVulnerabilities", "TruePositives", "TrueNegatives", "FalsePositives", "FalseNegatives"])
    for index, module in modules.iterrows():
        moduleData, moduleNumbers = checkModule(module["vendor"], module["module"], module["version"], module["vulnerableCVES"], database, comparator)
        comparisonDataResults.loc[len(comparisonDataResults)] = moduleData
        comparisonNumberResults.loc[len(comparisonNumberResults)] = moduleNumbers
        saveAnalysis(comparisonNumberResults, tempDir, experiment, "number", database["tolerance"])
        saveAnalysis(comparisonDataResults, tempDir, experiment, "data", database["tolerance"])
    return comparisonDataResults, comparisonNumberResults

def saveAnalysis(data, tempDir, experiment, detail, tol):
    # Save analysis results to a CSV file
    if not os.path.exists(tempDir):
        os.mkdir(tempDir)
    fullname = os.path.join(tempDir, experiment + '-' + detail + '-' + str(tol) + '.csv')
    data.to_csv(fullname)

def resultsAnalyzis(databaseExperiment):
    # Analyze the results of experiments
    totalModules = len(databaseExperiment)
    passModules = len(databaseExperiment[databaseExperiment["passComparison"] == True])
    failModules = len(databaseExperiment[databaseExperiment["passComparison"] == False])
    passRate = passModules / totalModules
    failRate = failModules / totalModules
    similarityMean = databaseExperiment[["similarity"]].mean()
    print("passRate: ", passRate, flush=True)
    print("failRate: ", failRate, flush=True)
    print("similarityMean: ", similarityMean, flush=True)
    return passRate, failRate, similarityMean

def runDatabaseAnalysis(database, outdir, experiment, modules, comparator):
    # Run analysis on multiple databases
    passRate = 0
    tolerance = 0
    for tol in range(int(database["tolerance"] * 100), int(database["limit"]), 10):
        cve_st = time.time()
        database["tolerance"] = tol / 100
        tempDir = outdir + '/' + database["db"]
        print("Analyzing ", database["db"], " with tolerance: ", tol, flush=True)
        resultData, resultNumber = runBulkModuleExperiment(modules, database, comparator, tempDir, experiment)
        means = resultNumber[["totalTime", "avgTime", "lenRealVulnerabilities", "lenTruePositives", "lenTrueNegatives", "lenFalsePositives", "lenFalseNegatives", "precision", "recall"]].mean()
        print(means)
        cve_et = time.time()
        elapsed_time = cve_et - cve_st
        print("Database Time: ", elapsed_time)

def readExperimentConfiguration(outdir, configFile, experiment, comparator=ComparatorType.HEURISTIC):
    # Read experiment configuration and run analysis
    modules = pd.read_csv(configFile)
    databases = [
        {"db": "database-human-1", "tolerance": 0.70, "limit": 75},
        {"db": "database-openai-2", "tolerance": 0.70, "limit": 75},
        {"db": "database-heuristic-4", "tolerance": 0.45, "limit": 65}
    ]
    print("----------------------------------------Start Experiment Execution ------------------------------------------------")
    print("Running Comparator: ", comparator, flush=True)
    if comparator == ComparatorType.OPENAIFULL:
        runDatabaseAnalysis(databases[0], outdir, experiment, modules, comparator)
    else:
        Parallel(n_jobs=3)(delayed(runDatabaseAnalysis)(database, outdir, experiment, modules, comparator) for database in databases)

def main():
    # Main function to run the script based on command-line arguments
    print(sys.argv)
    outdir = sys.argv[1]
    configFile = sys.argv[2]
    experiment = sys.argv[3]
    comparator = sys.argv[4]
    execute = False
    if comparator == "OPENAIFULL":
        comparator = ComparatorType.OPENAIFULL
        execute = True
    elif comparator == "OPENAIDB":
        comparator = ComparatorType.OPENAIDB
        execute = True
    elif comparator == "HEURISTIC":
        comparator = ComparatorType.HEURISTIC
        execute = True
    if execute:
        readExperimentConfiguration(outdir, configFile, experiment, comparator)

# readExperimentConfiguration('results/comparison', "experiment_02_modules.csv", "experiment-openaifull-2", ComparatorType.OPENAIFULL)
# readExperimentConfiguration('results/comparison', "experiment_02_modules.csv", "experiment-openaidb-2", ComparatorType.OPENAIDB)
# readExperimentConfiguration('results/comparison', "experiment_02_modules.csv", "experiment-heuristic-2", ComparatorType.HEURISTIC)

# python3 versions_comparator.py 'results/comparison' "shodan_modules_1.csv" "experiment-openaifull-2" "OPENAIFULL" > OPENAIFULL-2.log
# python versions_comparator.py 'results/comparison' "shodan_modules_1.csv" "experiment-openaidb-2" "OPENAIDB" > OPENAIDB-2.log
# python versions_comparator.py 'results/comparison' "shodan_modules_1.csv" "experiment-heuristic-2" "HEURISTIC" > HEURISTIC-2.log
