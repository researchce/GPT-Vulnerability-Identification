import re
import pymongo
import sys
import operator
import os
import ast
import pandas as pd
import time
from enum import Enum
from string_compare import *
import openai_cve_tester
from joblib import Parallel, delayed

"""
This script compares software module versions against CVE (Common Vulnerabilities and Exposures) data to determine
vulnerabilities using various algorithms and approaches. It includes functions to:
- Analyze manifests and compare versions.
- Fetch CVE data from a MongoDB database.
- Execute bulk module experiments and analyze results.
- Save results to files.
- Run database analysis and read experiment configurations.

Classes:
- ItemType: Enum class for different types of items in a manifest.
- MetricType: Enum class for different types of metrics.
- ComparatorType: Enum class for different comparison approaches.

Functions:
- analyzeManifest: Analyzes a version module against a CVE manifest.
- compareVersions: Compares versions directly or using a complete manifest.
- compare: Compares module versions using specified operators.
- compareEqVersions: Compares versions for equality.
- compareCompleteManifest: Compares versions using complete manifest data.
- checkVersionString: Checks and formats the version string.
- checkVulnerability: Determines if a module version is vulnerable based on the manifest.
- identifyElements: Identifies elements in a CVE manifest and tags them.
- getCvesMatchModule: Fetches CVE data matching a module from MongoDB.
- getRatio: Calculates match ratios using different algorithms.
- analyzeEachModule: Analyzes each module to determine vulnerability.
- executeAnalyzeModules: Executes module analysis using specified comparator.
- analyzeVulnerabilities: Analyzes vulnerabilities for a given module and version.
- checkModule: Checks a module against real vulnerabilities and returns metrics.
- getMetric: Calculates precision or recall based on true positives, false positives, and false negatives.
- runBulkModuleExperiment: Runs bulk experiments on modules and saves results.
- saveAnalysis: Saves analysis results to a file.
- resultsAnalyzis: Analyzes and prints overall results.
- runDatabaseAnalysis: Runs analysis on databases with different tolerance levels.
- readExperimentConfiguration: Reads experiment configuration and executes the analysis.
- main: Main function to execute the script.
"""

# Define operators for version comparison
ops = {
    '<' : operator.lt,
    '<=' : operator.le,
    '>' : operator.gt,
    '>=' : operator.ge,
    '=' : operator.eq,
    '==' : operator.eq,
}

class ItemType(Enum):
    OPERATOR = 'operator'
    VERSION = 'version'
    ALL = 'all'

class MetricType(Enum):
    RECALL = 'RECALL'
    PRECISION = 'PRECISION'
    F1 = 'F1'
    
class ComparatorType(Enum):
    HEURISTIC = 'HEURISTIC'
    OPENAIDB = 'OPENAI_USING_DB'
    OPENAIFULL = 'OPENAI_FULL'

def analyzeManifest(versionModule, cveManifest):
    # Analyzes a version module against a CVE manifest to determine vulnerability
    if type(cveManifest) == list:
        cveManifest = ' '.join(cveManifest)
    versionModule = versionModule.lower()
    cveManifest = cveManifest.lower()
    cveManifest = checkVersionString(cveManifest)
    cveManifest = cveManifest.split()
    cveManifest, manifestType = identifyElements(cveManifest)
    vulnerable = checkVulnerability(cveManifest, manifestType, versionModule)
    return vulnerable

def compareVersions(cveManifest, versionModule, directCompare=True):
    # Compares versions either directly or using a complete manifest
    vulnerable = False
    if not directCompare:
        vulnerable = compareCompleteManifest(cveManifest, versionModule)
    else:
        vulnerable = compareEqVersions(cveManifest, versionModule)
    return vulnerable

def compare(operator, cveVersion, moduleVersion):
    # Compares module versions using the specified operator
    return ops[operator](moduleVersion, cveVersion)

def compareEqVersions(cveManifest, versionModule):
    # Compares versions for equality
    vulnerable = False
    for item in cveManifest:
        if item["tag"] == ItemType.VERSION.value:
            currentVersion = item["item"].replace(" ", "")
            vulnerable = vulnerable or operator.eq(currentVersion, versionModule.replace(" ", ""))
    return vulnerable

def compareCompleteManifest(cveManifest, versionModule):
    # Compares versions using complete manifest data
    operatorReady = None
    vulnerable = False
    firstTime = True
    for idx, item in enumerate(cveManifest):
        if item["tag"] == ItemType.OPERATOR.value:
            operatorReady = item["item"]
        elif item["tag"] == ItemType.VERSION.value and operatorReady:
            if firstTime:
                vulnerable = True
                firstTime = False
            currentVersion = item["item"].replace(" ", "")
            vulnerable = vulnerable and compare(operatorReady.replace(" ", ""), currentVersion, versionModule.replace(" ", ""))
            operatorReady = None
        elif item["tag"] == ItemType.ALL.value and operatorReady:
            vulnerable = True
    return vulnerable

def checkVersionString(version):
    # Checks and formats the version string to include an operator if necessary
    operator = ''
    isValid = False
    currentOperatorIdx = 0
    for idx, letter in enumerate(version):
        if letter == '<' or letter == '>' or letter == '=':
            operator = operator + letter
            currentOperatorIdx = currentOperatorIdx + 1
        elif letter == ' ' and operator != '':
            break
        elif letter != ' ' and operator != '':
            isValid = True
            break
    res = operator + ' ' + version[idx:]
    if not isValid:
        res = version
    return res

def checkVulnerability(cveManifest, manifestType, versionModule):
    # Determines if a module version is vulnerable based on the manifest
    vulnerable = False
    if manifestType == ItemType.ALL:
        vulnerable = True
    elif manifestType == ItemType.VERSION:
        vulnerable = compareVersions(cveManifest, versionModule)
    elif manifestType == ItemType.OPERATOR:
        vulnerable = compareVersions(cveManifest, versionModule, directCompare=False)
    return vulnerable

def identifyElements(cveManifest):
    # Identifies elements in a CVE manifest and tags them as operator, version, or all
    cveManifestTag = []
    manifestType = ItemType.ALL
    for item in cveManifest:
        if re.match("[<><=>=]", item):
            cveManifestTag.append({"item": item, "tag": ItemType.OPERATOR.value})
            manifestType = ItemType.OPERATOR
        elif re.match(".*[0-9].*", item):
            cveManifestTag.append({"item": item, "tag": ItemType.VERSION.value})
            if manifestType != ItemType.OPERATOR:
                manifestType = ItemType.VERSION
        else:
            cveManifestTag.append({"item": item, "tag": ItemType.ALL.value})
    return cveManifestTag, manifestType

def getCvesMatchModule(module, databaseName):
    # Fetches CVE data matching a module from MongoDB
    myclient = pymongo.MongoClient("mongodb+srv://afelipevargasr:9TC5hlN9CdC3czP6@cluster0.lrohl7y.mongodb.net/?retryWrites=true&w=majority")
    mydb = myclient["cve_simplifications"]
    mycol = mydb[databaseName]
    myquery = { "vulnerable_versions.module": { "$regex": "(?i)" + module } }
    cves = mycol.find(myquery)
    return cves

def getRatio(cveModule, currentModule):
    # Calculates match ratios using different algorithms
    matchRatio = compare_string(cveModule, currentModule, AlgorithmType.MATCHER)
    matchRatioL = compare_string(cveModule, currentModule, AlgorithmType.LEVENSHTEIN)
    matchRatioC = compare_string(cveModule, currentModule, AlgorithmType.COSINE)
    print("MATCHER ", matchRatio, flush=True)
    print("LEVENSHTEIN ", matchRatioL, flush=True)
    print("COSINE ", matchRatioC, flush=True)
    return matchRatioC

def analyzeEachModule(cve, vendor, module, version, tolerationRate):
    # Analyzes each module to determine if it is vulnerable
    vulnerable = False
    passComparison = False
    for item in cve["vulnerable_versions"]:
        moduleName = module
        matchRatioC = getRatio(item["module"].lower(), moduleName)
        print(cve["CVE_ID"], flush=True)
        if matchRatioC < tolerationRate:
            moduleName = vendor + " " + module
            vendorMatchRatioC = getRatio(item["module"].lower(), moduleName)
            if vendorMatchRatioC >= tolerationRate:
                passComparison = True
        elif matchRatioC >= tolerationRate:
            passComparison = True
            
        if passComparison:
            print("tested version ", version, flush=True)
            print("module version ", item["versions"], flush=True)
            isModuleVulnerable = analyzeManifest(version, item["versions"])
            print("isModuleVulnerable ", isModuleVulnerable, flush=True)
            vulnerable = vulnerable or isModuleVulnerable
    return vulnerable

def executeAnalyzeModules(cve, vendor, module, version, tolerationRate, comparator):
    # Executes module analysis using the specified comparator
    vulnerable = False
    if comparator == ComparatorType.HEURISTIC:
        vulnerable = analyzeEachModule(cve, vendor, module, version, tolerationRate)
    elif comparator == ComparatorType.OPENAIDB:
        vulnerable = openai_cve_tester.check_database(
            cve["CVE_ID"],
            module, version,
            openai_cve_tester.ComparationType.VERSIONS,
            cve["vulnerable_versions"])
    elif comparator == ComparatorType.OPENAIFULL:
        vulnerable = openai_cve_tester.check_database(
            cve["CVE_ID"],
            module, version,
            openai_cve_tester.ComparationType.DESCRIPTION)
    return vulnerable

def analyzeVulnerabilities(vendor, module, version, databaseName, tolerationRate, comparator):
    # Analyzes vulnerabilities for a given module and version
    vendor = vendor.lower()
    module = module.lower()
    version = version.lower()
    cves = getCvesMatchModule(module, databaseName)
    cvesVulnerable = []
    cvesNotVulnerable = []
    totalTime = 0
    cves_count = 0
    avgTime = 0
    for cve in cves:
        cves_count += 1
        print("CVE: ", cve["CVE_ID"], flush=True)
        cve_st = time.time()
        vulnerable = executeAnalyzeModules(cve, vendor, module, version, tolerationRate, comparator)
        cve_et = time.time()
        elapsed_time = cve_et - cve_st
        totalTime += elapsed_time
        if vulnerable:
            cvesVulnerable.append(cve["CVE_ID"])
        else:
            cvesNotVulnerable.append(cve["CVE_ID"])
    print("lenCVES ", cves_count, flush=True)
    if cves_count > 0:
        avgTime = totalTime / cves_count
    return cvesVulnerable, cvesNotVulnerable, totalTime, avgTime

def checkModule(vendor, module, version, realVulnerabilities, database, comparator):
    # Checks a module against real vulnerabilities and returns metrics
    db = database["db"]
    tolerance = database["tolerance"]
    realVulnerabilities = ast.literal_eval(realVulnerabilities)
    vulnerableCVES, notVulnerableCVES, total, avg = analyzeVulnerabilities(vendor, module, version, db, tolerance, comparator)
    TruePositives = list(set(vulnerableCVES).intersection(realVulnerabilities))
    TrueNegatives = list(set(notVulnerableCVES) - set(realVulnerabilities))
    FalseNegatives = list(set(realVulnerabilities) - set(vulnerableCVES))
    FalsePositives = list(set(vulnerableCVES) - set(realVulnerabilities))
    print("realVulnerabilities ", realVulnerabilities, flush=True)
    print("vulnerableCVES ", vulnerableCVES, flush=True)
    print("notVulnerableCVES ", notVulnerableCVES, flush=True)
    print("TruePositives ", TruePositives, flush=True)
    print("TrueNegatives ", TrueNegatives, flush=True)
    print("FalseNegatives ", FalseNegatives, flush=True)
    print("FalsePositives ", FalsePositives, flush=True)
    passComparison = (realVulnerabilities == len(vulnerableCVES))
    
    resData = { 
        "db": database,
        "tolerance": tolerance,
        "comparator": comparator,
        "module": module, 
        "version": version,
        "realVulnerabilities": realVulnerabilities,
        "TruePositives": TruePositives,
        "TrueNegatives": TrueNegatives,
        "FalsePositives": FalsePositives,
        "FalseNegatives": FalseNegatives
    }
    resLen = { 
        "db": database,
        "tolerance": tolerance,
        "comparator": comparator,
        "module": module, 
        "version": version, 
        "totalTime": total,
        "avgTime": avg,
        "lenRealVulnerabilities": len(realVulnerabilities),
        "lenTruePositives": len(TruePositives),
        "lenTrueNegatives": len(TrueNegatives),
        "lenFalsePositives": len(FalsePositives),
        "lenFalseNegatives": len(FalseNegatives),
        "precision": getMetric(len(TruePositives), len(FalsePositives), len(FalseNegatives), MetricType.PRECISION),
        "recall": getMetric(len(TruePositives), len(FalsePositives), len(FalseNegatives), MetricType.RECALL),
        "passComparison": len(TruePositives) == len(realVulnerabilities)
    }
    return resData, resLen

def getMetric(TruePositives, FalsePositives, FalseNegatives, metric):
    # Calculates precision or recall based on true positives, false positives, and false negatives
    res = 0
    if TruePositives == 0 and FalseNegatives == 0 and FalsePositives == 0:
        res = 1
    elif (TruePositives > 0 or FalsePositives > 0) and metric == MetricType.PRECISION:
        res = TruePositives / (TruePositives + FalsePositives)
    elif (TruePositives > 0 or FalseNegatives > 0) and metric == MetricType.RECALL:
        res = TruePositives / (TruePositives + FalseNegatives)
    return res

def runBulkModuleExperiment(modules, database, comparator, tempDir, experiment):
    # Runs bulk experiments on modules and saves results
    comparisonNumberResults = pd.DataFrame(columns=[
        "db",
        "tolerance",
        "comparator",
        "module", 
        "version",
        "totalTime",
        "avgTime",
        "lenRealVulnerabilities",
        "lenTruePositives",
        "lenTrueNegatives",
        "lenFalsePositives",
        "lenFalseNegatives",
        "precision",
        "recall",
        "passComparison"
    ])
    comparisonDataResults = pd.DataFrame(columns=[
        "db",
        "tolerance",
        "comparator",
        "module", 
        "version",
        "realVulnerabilities",
        "TruePositives",
        "TrueNegatives",
        "FalsePositives",
        "FalseNegatives"
    ])
    for index, module in modules.iterrows():
        moduleData, moduleNumbers = checkModule(module["vendor"], module["module"], module["version"], module["vulnerableCVES"], database, comparator)
        comparisonDataResults.loc[len(comparisonDataResults)] = moduleData
        comparisonNumberResults.loc[len(comparisonNumberResults)] = moduleNumbers
        saveAnalysis(comparisonNumberResults, tempDir, experiment, "number", database["tolerance"])
        saveAnalysis(comparisonDataResults, tempDir, experiment, "data", database["tolerance"])
    return comparisonDataResults, comparisonNumberResults

def saveAnalysis(data, tempDir, experiment, detail, tol):
    # Saves analysis results to a file
    if not os.path.exists(tempDir):
        os.mkdir(tempDir)
    fullname = os.path.join(tempDir, experiment + '-' + detail + '-' + str(tol) + '.csv')
    data.to_csv(fullname)

def resultsAnalyzis(databaseExperiment):
    # Analyzes and prints overall results
    totalModules = len(databaseExperiment)
    passModules = len(databaseExperiment[databaseExperiment["passComparison"] == True])
    failModules = len(databaseExperiment[databaseExperiment["passComparison"] == False])
    passRate = passModules / totalModules
    failRate = failModules / totalModules
    similarityMean = databaseExperiment[["similarity"]].mean()
    print("passRate: ", passRate, flush=True)
    print("failRate: ", failRate, flush=True)
    print("similarityMean: ", similarityMean, flush=True)
    return passRate, failRate, similarityMean

def runDatabaseAnalysis(database, outdir, experiment, modules, comparator):
    # Runs analysis on databases with different tolerance levels
    passRate = 0
    tolerance = 0
    for tol in range(int(database["tolerance"] * 100), int(database["limit"]), 10):
        cve_st = time.time()
        database["tolerance"] = tol / 100
        tempDir = outdir + '/' + database["db"]
        print("Analyzing ", database["db"], " with tolerance: ", tol, flush=True)
        resultData, resultNumber = runBulkModuleExperiment(modules, database, comparator, tempDir, experiment)
        means = resultNumber[[
            "totalTime",
            "avgTime",
            "lenRealVulnerabilities",
            "lenTruePositives",
            "lenTrueNegatives",
            "lenFalsePositives",
            "lenFalseNegatives",
            "precision",
            "recall"
        ]].mean()
        print(means)
        cve_et = time.time()
        elapsed_time = cve_et - cve_st
        print("Database Time: ", elapsed_time)

def readExperimentConfiguration(outdir, configFile, experiment, comparator=ComparatorType.HEURISTIC):
    # Reads experiment configuration and executes the analysis
    modules = pd.read_csv(configFile)
    databases = [
        {"db": "database-human-1", "tolerance": 0.70, "limit": 75},
        {"db": "database-openai-2", "tolerance": 0.70, "limit": 75},
        {"db": "database-heuristic-4", "tolerance": 0.45, "limit": 65}
    ]
    print("----------------------------------------Start Experiment Execution ------------------------------------------------")
    print("Running Comparator: ", comparator, flush=True)
    if comparator == ComparatorType.OPENAIFULL:
        runDatabaseAnalysis(databases[0], outdir, experiment, modules, comparator)
    else:
        Parallel(n_jobs=3)(delayed(runDatabaseAnalysis)(database, outdir, experiment, modules, comparator) for database in databases)

def main():
    # Main function to execute the script
    print(sys.argv)
    outdir = sys.argv[1]
    configFile = sys.argv[2]
    experiment = sys.argv[3]
    comparator = sys.argv[4]
    execute = False
    if comparator == "OPENAIFULL":
        comparator = ComparatorType.OPENAIFULL
        execute = True
    elif comparator == "OPENAIDB":
        comparator = ComparatorType.OPENAIDB
        execute = True
    elif comparator == "HEURISTIC":
        comparator = ComparatorType.HEURISTIC
        execute = True
    if execute:
        readExperimentConfiguration(outdir, configFile, experiment, comparator)

main()

# Example function calls:
# readExperimentConfiguration('results/comparison', "experiment_02_modules.csv", "experiment-openaifull-2", ComparatorType.OPENAIFULL)
# readExperimentConfiguration('results/comparison', "experiment_02_modules.csv", "experiment-openaidb-2", ComparatorType.OPENAIDB)
# readExperimentConfiguration('results/comparison', "experiment_02_modules.csv", "experiment-heuristic-2", ComparatorType.HEURISTIC)

# Command-line execution examples:
# python3 versions_comparator.py 'results/comparison' "shodan_modules_1.csv" "experiment-openaifull-2" "OPENAIFULL" > OPENAIFULL-2.log
# python versions_comparator.py 'results/comparison' "shodan_modules_1.csv" "experiment-openaidb-2" "OPENAIDB" > OPENAIDB-2.log
# python versions_comparator.py 'results/comparison' "shodan_modules_1.csv" "experiment-heuristic-2" "HEURISTIC" > HEURISTIC-2.log
